{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c719ee8-bac5-4c5e-86d8-02bf5939cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW  # Use PyTorch's AdamW\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "from csibert_util import CSIBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6509dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "# Check MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b31f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c26be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e7c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multi-Cell CSI Data\n",
    "# multi_cell_data = scipy.io.loadmat('foundation_model_data/multi_cell_csi.mat')['csiMatrix']\n",
    "\n",
    "# Load Mobility CSI Data\n",
    "# mobility_data = scipy.io.loadmat('foundation_model_data/mobility_csi.mat')['csiTimeVarying']\n",
    "\n",
    "# Load MU-MIMO CSI Data\n",
    "# mu_mimo_data = scipy.io.loadmat('foundation_model_data/mu_mimo_csi.mat')['csiMU']\n",
    "\n",
    "# Load Heterogeneous Network Data\n",
    "# heterogeneous_data = scipy.io.loadmat('foundation_model_data/heterogeneous_network_csi.mat')\n",
    "# csi_macro = heterogeneous_data['csiMacro']\n",
    "# csi_small_cell = heterogeneous_data['csiSmallCell']\n",
    "\n",
    "cell_data = scipy.io.loadmat('foundation_model_data/csi_data_massive_mimo.mat')['multi_cell_csi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess individual CSI matrices\n",
    "def preprocess_csi_matrix(csi_matrix):\n",
    "    \"\"\"\n",
    "    Preprocess CSI data to handle variable-length sequences and retain time, frequency, and spatial dimensions.\n",
    "    \"\"\"\n",
    "    csi_real = np.real(csi_matrix)\n",
    "    csi_imag = np.imag(csi_matrix)\n",
    "\n",
    "    # Normalize real and imaginary parts\n",
    "    csi_real_normalized = (csi_real - np.mean(csi_real)) / np.std(csi_real)\n",
    "    csi_imag_normalized = (csi_imag - np.mean(csi_imag)) / np.std(csi_imag)\n",
    "\n",
    "    # Combine real and imaginary components\n",
    "    csi_combined = np.stack([csi_real_normalized, csi_imag_normalized], axis=-1)  # Shape: (subcarriers, Tx, Rx, 2)\n",
    "    \n",
    "    # Flatten freq, spatial, and real/imaginary into a single feature dimension\n",
    "    time_dim = csi_combined.shape[0]\n",
    "    feature_dim = np.prod(csi_combined.shape[1:])  # Tx × Rx × 2\n",
    "    csi_combined = csi_combined.reshape(time_dim, feature_dim)  # Shape: (time, feature_dim)\n",
    "    \n",
    "    return csi_combined\n",
    "\n",
    "# Traverse the nested cell structure\n",
    "preprocessed_data = []\n",
    "sequence_lengths = []\n",
    "\n",
    "for cell_idx in range(cell_data.shape[0]):  # Iterate over cells\n",
    "    for ue_idx in range(cell_data.shape[1]):  # Iterate over UEs\n",
    "        ue_data = cell_data[cell_idx, ue_idx]\n",
    "        for scenario in ue_data[0]:  # Each UE has multiple scenarios\n",
    "            processed_csi = preprocess_csi_matrix(scenario)\n",
    "            preprocessed_data.append(processed_csi)\n",
    "            sequence_lengths.append(processed_csi.shape[0])  # Track sequence lengths\n",
    "\n",
    "# Convert to padded 3D array (batch_size, sequence_length, feature_dim)\n",
    "max_sequence_length = max(sequence_lengths)\n",
    "feature_dim = preprocessed_data[0].shape[-1]\n",
    "\n",
    "# Pad sequences dynamically\n",
    "padded_data = np.zeros((len(preprocessed_data), max_sequence_length, feature_dim), dtype=np.float32)\n",
    "attention_masks = np.zeros((len(preprocessed_data), max_sequence_length), dtype=np.float32)\n",
    "for i, sequence in enumerate(preprocessed_data):\n",
    "    seq_len = sequence.shape[0]\n",
    "    padded_data[i, :seq_len, :] = sequence\n",
    "    attention_masks[i, :seq_len] = 1  # Mask for unpadded tokens\n",
    "\n",
    "# Mask data for masked signal prediction task\n",
    "def mask_data(data, mask_ratio=0.15):\n",
    "    mask = np.random.rand(*data.shape[:-1]) < mask_ratio  # Exclude the last dimension (real/imaginary parts)\n",
    "    masked_data = np.copy(data)\n",
    "    masked_data[mask, :] = 0  # Replace masked elements with 0\n",
    "    return masked_data, mask\n",
    "\n",
    "masked_data, mask = mask_data(padded_data)\n",
    "\n",
    "# Ensure consistent data types\n",
    "masked_data = masked_data.astype(np.float32)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963b30c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/1c0lrj_x0x956417g86lc4ph0000gn/T/ipykernel_28025/1218273177.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(attention_masks).float()  # Attention masks\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(masked_data).float(),  # Masked inputs\n",
    "    torch.tensor(padded_data).float(),  # Labels (original data)\n",
    "    torch.tensor(attention_masks).float()  # Attention masks\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define a directory to save the best model\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d03d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Optimizer\n",
    "model = CSIBERT(feature_dim=feature_dim)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=10000\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22aa9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d306943d3e14731ab938902dfb5cc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss 0.3055472834 at epoch 1\n",
      "New best model saved with loss 0.0267563988 at epoch 2\n",
      "New best model saved with loss 0.0207534327 at epoch 3\n",
      "New best model saved with loss 0.0167390905 at epoch 4\n",
      "New best model saved with loss 0.0151408155 at epoch 5\n",
      "New best model saved with loss 0.0144014567 at epoch 6\n",
      "New best model saved with loss 0.0139719130 at epoch 7\n",
      "New best model saved with loss 0.0135275266 at epoch 8\n",
      "New best model saved with loss 0.0132225568 at epoch 9\n",
      "New best model saved with loss 0.0129732331 at epoch 10\n",
      "New best model saved with loss 0.0126434842 at epoch 11\n",
      "New best model saved with loss 0.0125204573 at epoch 12\n",
      "New best model saved with loss 0.0122346366 at epoch 13\n",
      "New best model saved with loss 0.0120529235 at epoch 14\n",
      "New best model saved with loss 0.0119613288 at epoch 15\n",
      "New best model saved with loss 0.0118304521 at epoch 16\n",
      "New best model saved with loss 0.0117534283 at epoch 17\n",
      "New best model saved with loss 0.0116912153 at epoch 18\n",
      "New best model saved with loss 0.0115958911 at epoch 19\n",
      "New best model saved with loss 0.0115119104 at epoch 20\n",
      "New best model saved with loss 0.0114818359 at epoch 21\n",
      "New best model saved with loss 0.0114550222 at epoch 22\n",
      "New best model saved with loss 0.0114254339 at epoch 23\n",
      "New best model saved with loss 0.0113732258 at epoch 24\n",
      "New best model saved with loss 0.0113497740 at epoch 25\n",
      "New best model saved with loss 0.0113405769 at epoch 26\n",
      "New best model saved with loss 0.0113063754 at epoch 27\n",
      "New best model saved with loss 0.0112759086 at epoch 28\n",
      "New best model saved with loss 0.0112735848 at epoch 29\n",
      "New best model saved with loss 0.0112525682 at epoch 30\n",
      "New best model saved with loss 0.0112348343 at epoch 31\n",
      "New best model saved with loss 0.0112339257 at epoch 32\n",
      "New best model saved with loss 0.0112047960 at epoch 33\n",
      "New best model saved with loss 0.0111962990 at epoch 35\n",
      "New best model saved with loss 0.0111828224 at epoch 36\n",
      "New best model saved with loss 0.0111767669 at epoch 37\n",
      "New best model saved with loss 0.0111574333 at epoch 38\n",
      "New best model saved with loss 0.0111516619 at epoch 39\n",
      "New best model saved with loss 0.0111447592 at epoch 40\n",
      "New best model saved with loss 0.0111392837 at epoch 41\n",
      "New best model saved with loss 0.0111236779 at epoch 42\n",
      "New best model saved with loss 0.0111218286 at epoch 45\n",
      "New best model saved with loss 0.0111082187 at epoch 46\n",
      "New best model saved with loss 0.0111070883 at epoch 47\n",
      "New best model saved with loss 0.0110903284 at epoch 48\n",
      "New best model saved with loss 0.0110889863 at epoch 49\n",
      "New best model saved with loss 0.0110760731 at epoch 51\n",
      "New best model saved with loss 0.0110717656 at epoch 53\n",
      "New best model saved with loss 0.0110655149 at epoch 55\n",
      "New best model saved with loss 0.0110652589 at epoch 57\n",
      "New best model saved with loss 0.0110651990 at epoch 77\n",
      "New best model saved with loss 0.0110613952 at epoch 82\n",
      "New best model saved with loss 0.0110575633 at epoch 100\n",
      "New best model saved with loss 0.0110575253 at epoch 276\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 27\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/optim/adamw.py:471\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    469\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 471\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Variables to track the best model\n",
    "best_loss = float('inf')\n",
    "patience = 350\n",
    "patience_counter = 0\n",
    "\n",
    "loss_values = []  # Track loss over iterations\n",
    "# In[10]:\n",
    "for epoch in tqdm(range(1000000)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, labels, attention_mask = batch  # Add attention mask\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_values.append(avg_loss)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "        # Save the best model\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"best_model.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            \"feature_dim\": feature_dim,  # Save feature_dim\n",
    "            'loss': avg_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"New best model saved with loss {avg_loss:.10f} at epoch {epoch + 1}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Check if early stopping criteria are met\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}. Best loss: {best_loss:.10f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
